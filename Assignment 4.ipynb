{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnNMdpl5xYC3"
      },
      "source": [
        "###  Importing the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpLuhsrDx4bz"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import sklearn\r\n",
        "from sklearn import model_selection, datasets, preprocessing\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgG0vo0Twt4g"
      },
      "source": [
        "# A. Iris Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXrGroWTyWLv"
      },
      "source": [
        "### 1. Loading the Iris dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cRyJep-yiCf"
      },
      "source": [
        "x, y = datasets.load_iris (return_X_y=True) #Splitting the Iris data into x (features) and y (targets)."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKd9rPp-1z_w"
      },
      "source": [
        "### 2. Splitting the data and prepare it for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkiqj7Kp2JEP"
      },
      "source": [
        "#Splitting the data into 90% training set and 10% testing set.\r\n",
        "x_train, x_test, y_train , y_test = model_selection.train_test_split(x,y, test_size = 0.1, random_state = 42, stratify=y) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RsiQc04zMR_"
      },
      "source": [
        "### 3. Performing data preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eye2RMLF3Ag3"
      },
      "source": [
        "#Using QuantileTransformer() function to perform non-parametric transformation, to map the data to a uniform distribution with values between 0 and 1, to facilitate the training process.\r\n",
        "quantile_transformer = preprocessing.QuantileTransformer(random_state=42) \r\n",
        "\r\n",
        "#Using fit_transform() function on the training data to calculate the mean and variance.\r\n",
        "x_train_trans = quantile_transformer.fit_transform(x_train) \r\n",
        "#Using transform() function on the test data, so we can use the same mean and variance from the one calculated using fit_transform() function.\r\n",
        "x_test_trans = quantile_transformer.transform(x_test)\r\n",
        "\r\n",
        "#Exploring the data instances and shape so we can build our network based on those informations.\r\n",
        "print(x_train_trans.shape) \r\n",
        "print(y_train.shape)\r\n",
        "print(x_test_trans.shape)\r\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnC8NDApB7o3"
      },
      "source": [
        "### 4. Encoding the target (y) values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkOrHkxIB4_H"
      },
      "source": [
        "#Using one_hot() function encode the classes into a (0,1) to prevent the model from assuming a natural ordering between categories that may result in poor performance. \r\n",
        "y_train_onehot, y_test_onehot = tf.one_hot(y_train, depth=len(set(y_train))), tf.one_hot(y_test, depth=len(set(y_test)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ7Lk1MEILye"
      },
      "source": [
        "### 5. Building, Compiling and Training the classification model using deep neural networks:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_Mbc4dCI2-e"
      },
      "source": [
        "#Deleting tensorflow and importing it again to prevent the model from the saving the weights after training.\r\n",
        "del tf \r\n",
        "import tensorflow as tf \r\n",
        "\r\n",
        "tf.random.set_seed(42) #Setting the graph-level random seed to get the same random numbers at every session (reproducible results). \r\n",
        "\r\n",
        "#Building the model using tensorflow's Sequential API for easier implementation.\r\n",
        "model = tf.keras.Sequential([\r\n",
        "                             tf.keras.layers.Input(shape=[4]), #Input layer with 4 units to hold the 4 features from the dataset.\r\n",
        "                             tf.keras.layers.Dense(10, activation=tf.nn.leaky_relu,), #Hidden layer with 10 units to be trained, with leaky_relu to eleminate linearity.\r\n",
        "                             tf.keras.layers.Dense(3, activation=tf.nn.softmax) #Output layer with 3 units to represent the 3 classes of the dataset.\r\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_F2n3Q0qU60"
      },
      "source": [
        "#Compiling the model by using Adam() as the optimizer and CategoricalCrossentropy() as this is a multi-class problem.\r\n",
        "model.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.CategoricalCrossentropy(), metrics=['acc'])\r\n",
        "\r\n",
        "#Training the model.\r\n",
        "history = model.fit(x=x_train_trans, y=y_train_onehot, batch_size = 16, epochs=250, validation_data=(x_test_trans, y_test_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biyuFE6zLZce"
      },
      "source": [
        "### 6. Plotting the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZYvf1VWLivW"
      },
      "source": [
        "#Plotting the results to look for overfitting within the results.\r\n",
        "#Converting the results into a data frame using Pandas tool.\r\n",
        "pd.DataFrame(history.history).plot(figsize=(10, 7))\r\n",
        "\r\n",
        "#Plotting the data frame using matplotlib library.\r\n",
        "plt.grid(True)\r\n",
        "plt.gca().set_ylim(0, 1.2) #Setting the x-axis.\r\n",
        "plt.gca().set_xlim(0,249) #Setting the y-axis.\r\n",
        "                             \r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxJwOOUoNip-"
      },
      "source": [
        "### 7. Model evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJEIEcXtNi4d"
      },
      "source": [
        "#Evaluating the model on the test (validation) data samples.\r\n",
        "print('Model Loss and Accuracy:', model.evaluate(x_test_trans, y_test_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjroN9huIFFH"
      },
      "source": [
        "### Notes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMd292cZi_2U"
      },
      "source": [
        "1. StandardScaler() was not used, as the model did not generalize well.\r\n",
        "\r\n",
        "2. QuantileTransformer() showed the best results.\r\n",
        "\r\n",
        "3. Many architectures were used to train the model, (1 hidden, with 8 or 16 or 32 or 64), (2 hidden with 8 or 16 or 32 or 64)...etc. \r\n",
        "\r\n",
        "4. Different batche sizes were also used (16, 32, 64).\r\n",
        "\r\n",
        "5. Using 10 units with one hidden layer and a batch of 16, showed the best results.\r\n",
        "\r\n",
        "6. A network with only one hidden layer and two units was used, but it took 2500 epochs to reach a loss of 0.2 without overfitting.\r\n",
        "\r\n",
        "7. Dropout layer was not used in the model, as it affected the results badly with every rate (0.1, 0.2, 0.3 ...1.0), the validation loss curve was noticed to experience a lot of spiking with the addition of Dropout layer.\r\n",
        "\r\n",
        "8. Callbacks (EarlyStopping), was also not used, because the model was eventually converging after a certain point and the function was stopping the model at an early stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH0un5-JmU4U"
      },
      "source": [
        "# B. Wine Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDKLJDdJn-NT"
      },
      "source": [
        "### 1. Loading the Wine dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cee2cA9Ym--6"
      },
      "source": [
        "x, y = datasets.load_wine (return_X_y=True) #Splitting the Iris data into x (features) and y (targets)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRP6VTtHoYfy"
      },
      "source": [
        "### 2. Performing data normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjEuODBWovZr"
      },
      "source": [
        "x = StandardScaler().fit(x).transform(x) #Normalizing the data to get the best results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuVZu4aBoNCU"
      },
      "source": [
        "### 3. Splitting the data and prepare it for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5gZsQIXoTIz"
      },
      "source": [
        "#Splitting the data into 90% training set and 10% testing set.\r\n",
        "x_train, x_test, y_train , y_test = model_selection.train_test_split(x,y, test_size = 0.1, random_state = 42, stratify=y)\r\n",
        "\r\n",
        "#Exploring the data instances and shape so we can build our network based on those informations.\r\n",
        "print(x_train.shape) \r\n",
        "print(y_train.shape)\r\n",
        "print(x_test.shape)\r\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bmFij_wpby7"
      },
      "source": [
        "### 4. Encoding the target (y) values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqBV7iurphGL"
      },
      "source": [
        "#Using one_hot() function encode the classes into a (0,1) to prevent the model from assuming a natural ordering between categories that may result in poor performance. \r\n",
        "y_train_onehot, y_test_onehot = tf.one_hot(y_train, depth=len(set(y_train))), tf.one_hot(y_test, depth=len(set(y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-9bJ663plxD"
      },
      "source": [
        "### 5. Building, Compiling and Training the classification model using deep neural networks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVVpcaZ_pm97"
      },
      "source": [
        "#Deleting tensorflow and importing it again to prevent the model from the saving the weights after training.\r\n",
        "del tf \r\n",
        "import tensorflow as tf \r\n",
        "\r\n",
        "tf.random.set_seed(42) #Setting the graph-level random seed to get the same random numbers at every session (reproducible results). \r\n",
        "\r\n",
        "#Building the model using tensorflow's Sequential API for easier implementation.\r\n",
        "model = tf.keras.Sequential([\r\n",
        "                             tf.keras.layers.Input(shape=[13]), #Input layer with 4 units to hold the 4 features from the dataset.\r\n",
        "                             tf.keras.layers.Dense(8, activation=tf.nn.leaky_relu,), #Hidden layer with 10 units to be trained, with leaky_relu to eleminate linearity.\r\n",
        "                             tf.keras.layers.Dense(3, activation=tf.nn.softmax) #Output layer with 3 units to represent the 3 classes of the dataset.\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j6JAf5AqReb"
      },
      "source": [
        "#Compiling the model by using Adam() as the optimizer and CategoricalCrossentropy() as this is a multi-class problem.\r\n",
        "model.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.CategoricalCrossentropy(), metrics=['acc'])\r\n",
        "\r\n",
        "#Training the model.\r\n",
        "history = model.fit(x=x_train, y=y_train_onehot, batch_size = 64, epochs=250, validation_data=(x_test, y_test_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp5x3_Zaqb2D"
      },
      "source": [
        "### 6. Plotting the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkCyIPuDqf6z"
      },
      "source": [
        "#Plotting the results to look for overfitting within the results.\r\n",
        "#Converting the results into a data frame using Pandas tool.\r\n",
        "pd.DataFrame(history.history).plot(figsize=(10, 7))\r\n",
        "\r\n",
        "#Plotting the data frame using matplotlib library.\r\n",
        "plt.grid(True)\r\n",
        "plt.gca().set_ylim(0, 1.2) #Setting the x-axis.\r\n",
        "plt.gca().set_xlim(0,249) #Setting the y-axis.\r\n",
        "                             \r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbZmhqBKqihb"
      },
      "source": [
        "### 7. Model evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_dlbxRdqx_9"
      },
      "source": [
        "#Evaluating the model on the test (validation) data samples.\r\n",
        "print('Model Loss and Accuracy:', model.evaluate(x_test_trans, y_test_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJv-FNOoq4-z"
      },
      "source": [
        "### Notes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpOkdBR-q7U7"
      },
      "source": [
        "1. Many architectures were used to train the model, (1 hidden, with 8 or 16 or 32 or 64), (2 hidden with 8 or 16 or 32 or 64)...etc.\r\n",
        "\r\n",
        "2. Different batche sizes were also used (16, 32, 64).\r\n",
        "\r\n",
        "3. Using 8 units with one hidden layer and a batch of 64, showed the best results.\r\n",
        "\r\n",
        "4. Dropout layer was not used in the model, as it affected the results badly with every rate (0.1, 0.2, 0.3 ...1.0), the validation loss curve was noticed to experience a lot of spiking with the addition of Dropout layer.\r\n",
        "\r\n",
        "5. Callbacks (EarlyStopping), was also not used, because the model was eventually converging after a certain point and the function was stopping the model at an early stage."
      ]
    }
  ]
}